batch_size:                   16                                  # originally 32
distributed:                  False
num_epochs:                   40                                  # originally 8
accum_iter:                   10
accum_step:                   10                                  # originally 40 but shrank due to small training corpus
base_lr:                      1.0
max_padding:                  72
warmup:                       3000
model_path:                   "model/"
model_prefix:                 "symbolicTransformer_model_"
model_suffix:                 "final.pt"
vocab_file_name:              "vocab.pt"
layers:                       2                                 # originally 6
dimension:                    512
feed_forward_dimension:       2048
h_attention_layers:           8                                 # h=8 parallel attention layers
dropout:                      0.1
application_path:             "/"
remove_punctuation:           False                             # todo : implement, check with quotes signs for instance
target_label_smoothing:       0.1                               # velocity of label smoothing
optimizer_weight_decay:       0                                 # L2 penalty
log_in_database:              False                             # todo: create tables to receive losses, ... by hyper-parameters
beam_search:                  0                                 # 0 = greedy search & 1 = beam search
beam:
    beam-size:                5
    max-decoding-time-step:  70
