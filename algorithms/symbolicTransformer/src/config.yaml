learning:
    split_factor:             9

layers:                       5                                 # originally 6
batch_size:                   16                                 # Annotated transformer choose 32
num_epochs:                   20                                # Annotated transformer choose 8
dimension:                    512                               # Model dimension {attention, feed_forward, positional_encoding, encoder_layer, decoder_layer, embeddings, generator}
feed_forward_dimension:       2048
h_attention_layers:           8                                 # h=8 parallel attention layers
dropout:                      0.1
accum_iter:                   10                                # doing a "step" optimizer each n iterations (Originally 10)
accum_step:                   10                                # Write loss only each n steps! Originally 40 but shrank due to small training corpus
KL_divergence_loss:           True                              # True Kullback-Leibler divergence loss criterion, False similarity
warmup:                       3000
base_lr:                      1.0
target_label_smoothing:       0.8                               # velocity of label smoothing
optimizer_weight_decay:       0                                 # L2 penalty # todo: create tables to receive losses, ... by hyper-parameters
max_padding:                  72
output_max_words:             80                                # originally 80

remove_punctuation:           False                             # todo : implement, check with quotes signs for instance

configuration_path:
    xlsx_path:                "data/other_conte/xlsx/"
    csv_path:                 "data/conte/source_csv/"
    selected_db:              "db_dev"

application_path:             "/"
model_path:                   "model/"
model_prefix:                 "symbolicTransformer_model_"
model_suffix:                 "final.pt"
vocab_file_name:              "vocab.pt"
architecture_dev_mode:        True                              # True for "EN", False for "GLOSS"

using_gpu:                    True                              # for training, decoding use parameters. GPU = True and CPU = False
distributed:                  False                             # Annotated transformer choose False (for more than one processing unit)

fast_text_corpus:             True                              # Spacy if false
persist_learning_measure:     False                             # Write measures in database
beam_search:                  True                              # False for greedy search
beam:
    beam-size:                10
    max-decoding-time-step:   70
